__host__ ​cudaError_t cudaChooseDevice ( int* device, const cudaDeviceProp* prop )
Select compute-device which best matches criteria.
__host__ ​ __device__ ​cudaError_t cudaDeviceGetAttribute ( int* value, cudaDeviceAttr attr, int  device )
Returns information about the device.
__host__ ​cudaError_t cudaDeviceGetByPCIBusId ( int* device, const char* pciBusId )
Returns a handle to a compute device.
__host__ ​ __device__ ​cudaError_t cudaDeviceGetCacheConfig ( cudaFuncCache ** pCacheConfig )
Returns the preferred cache configuration for the current device.
__host__ ​ __device__ ​cudaError_t cudaDeviceGetLimit ( size_t* pValue, cudaLimit limit )
Returns resource limits.
__host__ ​cudaError_t cudaDeviceGetPCIBusId ( char* pciBusId, int  len, int  device )
Returns a PCI Bus Id string for the device.
__host__ ​ __device__ ​cudaError_t cudaDeviceGetSharedMemConfig ( cudaSharedMemConfig ** pConfig )
Returns the shared memory configuration for the current device.
__host__ ​cudaError_t cudaDeviceGetStreamPriorityRange ( int* leastPriority, int* greatestPriority )
Returns numerical values that correspond to the least and greatest stream priorities.
__host__ ​cudaError_t cudaDeviceReset ( void )
Destroy all allocations and reset all state on the current device in the current process.
__host__ ​cudaError_t cudaDeviceSetCacheConfig ( cudaFuncCache cacheConfig )
Sets the preferred cache configuration for the current device.
__host__ ​cudaError_t cudaDeviceSetLimit ( cudaLimit limit, size_t value )
Set resource limits.
__host__ ​cudaError_t cudaDeviceSetSharedMemConfig ( cudaSharedMemConfig config )
Sets the shared memory configuration for the current device.
__host__ ​ __device__ ​cudaError_t cudaDeviceSynchronize ( void )
Wait for compute device to finish.
__host__ ​ __device__ ​cudaError_t cudaGetDevice ( int* device )
Returns which device is currently being used.
__host__ ​ __device__ ​cudaError_t cudaGetDeviceCount ( int* count )
Returns the number of compute-capable devices.
__host__ ​cudaError_t cudaGetDeviceFlags ( unsigned int* flags )
Gets the flags for the current device.
__host__ ​cudaError_t cudaGetDeviceProperties ( cudaDeviceProp* prop, int  device )
Returns information about the compute-device.
__host__ ​cudaError_t cudaIpcCloseMemHandle ( void* devPtr )
Close memory mapped with cudaIpcOpenMemHandle.
__host__ ​cudaError_t cudaIpcGetEventHandle ( cudaIpcEventHandle_t* handle, cudaEvent_t event )
Gets an interprocess handle for a previously allocated event.
__host__ ​cudaError_t cudaIpcGetMemHandle ( cudaIpcMemHandle_t* handle, void* devPtr )
Gets an interprocess memory handle for an existing device memory allocation.
__host__ ​cudaError_t cudaIpcOpenEventHandle ( cudaEvent_t* event, cudaIpcEventHandle_t handle )
Opens an interprocess event handle for use in the current process.
__host__ ​cudaError_t cudaIpcOpenMemHandle ( void** devPtr, cudaIpcMemHandle_t handle, unsigned int  flags )
Opens an interprocess memory handle exported from another process and returns a device pointer usable in the local process.
__host__ ​cudaError_t cudaSetDevice ( int  device )
Set device to be used for GPU executions.
__host__ ​cudaError_t cudaSetDeviceFlags ( unsigned int  flags )
Sets flags to be used for device executions.
__host__ ​cudaError_t cudaSetValidDevices ( int* device_arr, int  len )
Set a list of devices that can be used for CUDA.


Read more at: http://docs.nvidia.com/cuda/cuda-runtime-api/index.html#ixzz3zPCk6fEF 
Follow us: @GPUComputing on Twitter | NVIDIA on Facebook